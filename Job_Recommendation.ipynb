{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## *1) NUMERIC FEATURES PIPELINE*"
      ],
      "metadata": {
        "id": "Z8xAGg-2bgQW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0RvGu7ibRCR"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# NUMERIC FEATURES PIPELINE\n",
        "# ========================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import umap.umap_ as umap\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# 1. Load Dataset & EDA\n",
        "# ========================================\n",
        "df = pd.read_csv(\"glassdoor_job_reviews.csv\")   # path to Kaggle dataset\n",
        "\n",
        "# Numeric features\n",
        "numeric_cols = [\"rating_overall\",\"career_opportunity\",\"compensation\",\n",
        "                \"culture_values\",\"senior_management\",\"work_life_balance\"]\n",
        "\n",
        "X = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
        "y = df[\"label\"].map({\"Negative\":0,\"No Opinion\":1,\"Positive\":2})\n",
        "\n",
        "# EDA plots\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.countplot(x=y)\n",
        "plt.title(\"Label Distribution\")\n",
        "plt.show()\n",
        "\n",
        "sns.heatmap(X.corr(),annot=True,cmap=\"Blues\")\n",
        "plt.title(\"Correlation Heatmap of Numeric Features\")\n",
        "plt.show()\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train,X_val,y_train,y_val = train_test_split(X_scaled,y,test_size=0.2,random_state=42)\n"
      ],
      "metadata": {
        "id": "35VcTVwQbcNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# 2. Numeric Baseline Models (LSTM/BiLSTM)\n",
        "# ========================================\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self,input_dim,hidden_dim,num_classes):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim,hidden_dim,batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim,num_classes)\n",
        "    def forward(self,x):\n",
        "        out,_ = self.lstm(x)\n",
        "        out = self.fc(out[:,-1,:])\n",
        "        return out\n",
        "\n",
        "class BiLSTMModel(nn.Module):\n",
        "    def __init__(self,input_dim,hidden_dim,num_classes):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim,hidden_dim,bidirectional=True,batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim*2,num_classes)\n",
        "    def forward(self,x):\n",
        "        out,_ = self.lstm(x)\n",
        "        out = self.fc(out[:,-1,:])\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "QrrELyYSbaLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# 3. Train & Eval Function\n",
        "# ========================================\n",
        "def train_model(model,train_loader,val_loader,epochs=10,lr=0.001):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "    opt = optim.Adam(model.parameters(),lr=lr)\n",
        "\n",
        "    train_losses,val_losses=[],[]\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        model.train(); loss_sum=0\n",
        "        for xb,yb in train_loader:\n",
        "            xb,yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            out = model(xb)\n",
        "            loss = crit(out,yb)\n",
        "            loss.backward(); opt.step()\n",
        "            loss_sum += loss.item()\n",
        "        train_losses.append(loss_sum/len(train_loader))\n",
        "\n",
        "        model.eval(); vloss=0\n",
        "        with torch.no_grad():\n",
        "            for xb,yb in val_loader:\n",
        "                xb,yb = xb.to(device), yb.to(device)\n",
        "                out = model(xb); loss=crit(out,yb); vloss+=loss.item()\n",
        "        val_losses.append(vloss/len(val_loader))\n",
        "        print(f\"Epoch {ep+1}: Train {train_losses[-1]:.4f}, Val {val_losses[-1]:.4f}\")\n",
        "    return train_losses,val_losses\n",
        "\n",
        "# Prepare DataLoaders\n",
        "train_loader = DataLoader(TensorDataset(torch.tensor(X_train).float().unsqueeze(1),\n",
        "                                        torch.tensor(y_train.values)),batch_size=32,shuffle=True)\n",
        "val_loader = DataLoader(TensorDataset(torch.tensor(X_val).float().unsqueeze(1),\n",
        "                                      torch.tensor(y_val.values)),batch_size=32)\n",
        "\n",
        "# Train BiLSTM\n",
        "bilstm = BiLSTMModel(len(numeric_cols),64,3)\n",
        "train_losses,val_losses = train_model(bilstm,train_loader,val_loader,epochs=5)\n",
        "lt.title(\"UMAP (Numeric)\"); plt.show()\n"
      ],
      "metadata": {
        "id": "_2LUkHmlbWri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ========================================\n",
        "# 4. Evaluation & Visualization\n",
        "# ========================================\n",
        "bilstm.eval()\n",
        "y_pred,y_true=[],[]\n",
        "with torch.no_grad():\n",
        "    for xb,yb in val_loader:\n",
        "        out = bilstm(xb)\n",
        "        preds = torch.argmax(out,dim=1)\n",
        "        y_pred.extend(preds.numpy()); y_true.extend(yb.numpy())\n",
        "\n",
        "print(classification_report(y_true,y_pred))\n",
        "cm = confusion_matrix(y_true,y_pred)\n",
        "sns.heatmap(cm,annot=True,fmt=\"d\",cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix - Numeric BiLSTM\")\n",
        "plt.show()\n",
        "\n",
        "# Accuracy/Loss curve\n",
        "plt.plot(train_losses,label=\"Train Loss\"); plt.plot(val_losses,label=\"Val Loss\")\n",
        "plt.legend(); plt.title(\"Loss Curve - BiLSTM\"); plt.show()\n",
        "\n",
        "# Embedding visualization (TSNE, UMAP)\n",
        "embeddings = bilstm(torch.tensor(X_val).float().unsqueeze(1)).detach().numpy()\n",
        "tsne = TSNE(n_components=2).fit_transform(embeddings)\n",
        "umap_emb = umap.UMAP().fit_transform(embeddings)\n",
        "\n",
        "plt.scatter(tsne[:,0],tsne[:,1],c=y_val,cmap=\"viridis\"); plt.title(\"t-SNE (Numeric)\"); plt.show()\n",
        "plt.scatter(umap_emb[:,0],umap_emb[:,1],c=y_val,cmap=\"plasma\"); p"
      ],
      "metadata": {
        "id": "a1ExFDlZbT2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# 1. Load Libraries\n",
        "# ========================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import umap.umap_ as umap\n",
        "\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, HANConv\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# ========================================\n",
        "# 2. Load Dataset\n",
        "# ========================================\n",
        "df = pd.read_csv(\"glassdoor_reviews.csv\")\n",
        "\n",
        "numeric_cols = [\"Career Opportunities\",\"Comp & Benefits\",\"Culture & Values\",\n",
        "                \"Senior Management\",\"Work/Life Balance\",\"Rating\"]\n",
        "\n",
        "df = df[numeric_cols].dropna()\n",
        "\n",
        "# Map labels from overall Rating\n",
        "def map_sentiment(r):\n",
        "    if r <= 2: return \"Negative\"\n",
        "    elif r == 3: return \"No Opinion\"\n",
        "    else: return \"Positive\"\n",
        "\n",
        "df[\"Label\"] = df[\"Rating\"].apply(map_sentiment)\n",
        "\n",
        "# ========================================\n",
        "# 3. Preprocessing\n",
        "# ========================================\n",
        "X = df.drop(columns=[\"Rating\",\"Label\"])\n",
        "y = df[\"Label\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_enc = le.fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_enc, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_t = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test_t, y_test_t), batch_size=32)\n",
        "\n",
        "# ========================================\n",
        "# 4. Baseline Models (LSTM, BiLSTM)\n",
        "# ========================================\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, bi=False):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=bi)\n",
        "        self.fc = nn.Linear(hidden_dim*(2 if bi else 1), output_dim)\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # seq length=1\n",
        "        _, (h, _) = self.lstm(x)\n",
        "        if self.lstm.bidirectional:\n",
        "            h = torch.cat((h[-2], h[-1]), dim=1)\n",
        "        else:\n",
        "            h = h[-1]\n",
        "        return self.fc(h)\n",
        "\n",
        "def train_model(model, train_loader, test_loader, epochs=10):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    history = {\"train_loss\":[],\"val_loss\":[]}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for xb, yb in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            out = model(xb)\n",
        "            loss = criterion(out, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        history[\"train_loss\"].append(total_loss/len(train_loader))\n",
        "\n",
        "        model.eval()\n",
        "        val_loss=0\n",
        "        with torch.no_grad():\n",
        "            for xb,yb in test_loader:\n",
        "                out=model(xb)\n",
        "                val_loss+=criterion(out,yb).item()\n",
        "        history[\"val_loss\"].append(val_loss/len(test_loader))\n",
        "        print(f\"Epoch {epoch+1}: Train Loss={history['train_loss'][-1]:.4f} Val Loss={history['val_loss'][-1]:.4f}\")\n",
        "    return history\n",
        "\n",
        "# Train LSTM\n",
        "lstm_model = LSTMModel(input_dim=X_train.shape[1], hidden_dim=32, output_dim=3)\n",
        "train_model(lstm_model, train_loader, test_loader, epochs=5)\n",
        "\n",
        "# Train BiLSTM\n",
        "bilstm_model = LSTMModel(input_dim=X_train.shape[1], hidden_dim=32, output_dim=3, bi=True)\n",
        "train_model(bilstm_model, train_loader, test_loader, epochs=5)\n",
        "\n",
        "# ========================================\n",
        "# 5. Transformer Baselines (BERT, RoBERTa, DistilBERT)\n",
        "# ========================================\n",
        "# Numeric features must be tokenized into pseudo-text or fed via tab-transformer\n",
        "# Here: simulate using MLP baseline instead\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "    def forward(self,x):\n",
        "        return self.fc2(torch.relu(self.fc1(x)))\n",
        "\n",
        "mlp_model = MLP(X_train.shape[1], 64, 3)\n",
        "train_model(mlp_model, train_loader, test_loader, epochs=5)\n",
        "\n",
        "# ========================================\n",
        "# 6. Proposed GNN\n",
        "# ========================================\n",
        "# Build simple kNN graph from numeric features\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "adj = kneighbors_graph(X_scaled, n_neighbors=5).tocoo()\n",
        "edge_index = torch.tensor([adj.row, adj.col], dtype=torch.long)\n",
        "\n",
        "x = torch.tensor(X_scaled, dtype=torch.float32)\n",
        "y_all = torch.tensor(y_enc, dtype=torch.long)\n",
        "\n",
        "class GNN(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "gnn_model = GNN(X_train.shape[1], 32, 3)\n",
        "optimizer = optim.Adam(gnn_model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    gnn_model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = gnn_model(x, edge_index)\n",
        "    loss = criterion(out[:len(X_train)], y_train_t)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss={loss.item():.4f}\")\n",
        "\n",
        "# ========================================\n",
        "# 7. Evaluation & Confusion Matrix\n",
        "# ========================================\n",
        "gnn_model.eval()\n",
        "pred = out[len(X_train):].argmax(dim=1)\n",
        "print(classification_report(y_test, pred, target_names=le.classes_))\n",
        "\n",
        "cm = confusion_matrix(y_test, pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.show()\n",
        "\n",
        "# ========================================\n",
        "# 8. Training/Validation Curves\n",
        "# (example from earlier history dictionary)\n",
        "# ========================================\n",
        "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
        "plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
        "plt.legend(); plt.show()\n",
        "\n",
        "# ========================================\n",
        "# 9. Embedding Visualizations (t-SNE, UMAP)\n",
        "# ========================================\n",
        "embeddings = out.detach().numpy()\n",
        "tsne = TSNE(n_components=2).fit_transform(embeddings)\n",
        "plt.scatter(tsne[:,0], tsne[:,1], c=y_enc, cmap=\"coolwarm\"); plt.title(\"t-SNE Embeddings\"); plt.show()\n",
        "\n",
        "umap_emb = umap.UMAP(n_components=2).fit_transform(embeddings)\n",
        "plt.scatter(umap_emb[:,0], umap_emb[:,1], c=y_enc, cmap=\"viridis\"); plt.title(\"UMAP Embeddings\"); plt.show()\n",
        "\n",
        "# ========================================\n",
        "# 10. Graph Node Visualization\n",
        "# ========================================\n",
        "import networkx as nx\n",
        "G = nx.from_scipy_sparse_matrix(adj)\n",
        "plt.figure(figsize=(6,6))\n",
        "nx.draw(G, node_color=y_enc, node_size=20, cmap=plt.cm.Set1)\n",
        "plt.title(\"Graph Node Visualization\")\n",
        "plt.show()\n",
        "\n",
        "# ========================================\n",
        "# 11. Confidence Predictions + Job Recommendations\n",
        "# ========================================\n",
        "probs = torch.softmax(out, dim=1)\n",
        "confidence = probs.max(dim=1).values\n",
        "pred_labels = probs.argmax(dim=1)\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"Sample {i}: True={le.classes_[y_enc[i]]}, Pred={le.classes_[pred_labels[i]]}, Conf={confidence[i].item():.2f}\")"
      ],
      "metadata": {
        "id": "8Q0cOj2ge5jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***2) TEXTUAL FEATURES PIPELINE***"
      ],
      "metadata": {
        "id": "kVvJzzWdbmPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# TEXTUAL FEATURES PIPELINE\n",
        "# ========================================\n",
        "import torch\n",
        "from transformers import BertTokenizer,BertModel,RobertaTokenizer,RobertaModel,DistilBertTokenizer,DistilBertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns; import matplotlib.pyplot as plt\n",
        "\n",
        "# Load reviews\n",
        "texts = df[\"review_text\"].fillna(\" \").tolist()\n",
        "labels = df[\"label\"].map({\"Negative\":0,\"No Opinion\":1,\"Positive\":2}).tolist()\n",
        "\n",
        "# Split\n",
        "X_train,X_val,y_train,y_val = train_test_split(texts,labels,test_size=0.2,random_state=42)\n",
        "\n",
        "# Example: BERT embeddings\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "inputs = tokenizer(X_val,return_tensors=\"pt\",padding=True,truncation=True,max_length=128)\n",
        "with torch.no_grad():\n",
        "    embeddings = bert_model(**inputs).last_hidden_state[:,0,:]\n",
        "\n",
        "# Classification layer\n",
        "clf = nn.Linear(embeddings.shape[1],3)\n",
        "outputs = clf(embeddings)\n",
        "\n",
        "preds = torch.argmax(outputs,dim=1).numpy()\n",
        "print(classification_report(y_val,preds))\n",
        "\n",
        "cm = confusion_matrix(y_val,preds)\n",
        "sns.heatmap(cm,annot=True,fmt=\"d\",cmap=\"Blues\"); plt.title(\"Confusion Matrix - BERT\"); plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "IHXiCzU0beD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# Graph Construction (HAN, GraphSAGE, GNN)\n",
        "# ========================================\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data\n",
        "import networkx as nx\n",
        "\n",
        "# Simple job-firm-review graph\n",
        "edge_index = torch.tensor([[0,1,2,2],[1,2,0,1]],dtype=torch.long)\n",
        "x = embeddings[:3]  # dummy node features\n",
        "graph_data = Data(x=x,edge_index=edge_index)\n",
        "\n",
        "# HAN Model\n",
        "import torch_geometric.nn as pyg_nn\n",
        "class HAN(torch.nn.Module):\n",
        "    def __init__(self,in_dim,hid_dim,out_dim):\n",
        "        super().__init__()\n",
        "        self.conv = pyg_nn.GATConv(in_dim,hid_dim,heads=2)\n",
        "        self.fc = nn.Linear(hid_dim*2,out_dim)\n",
        "    def forward(self,x,edge_index):\n",
        "        h = self.conv(x,edge_index)\n",
        "        return self.fc(h)\n",
        "\n",
        "han = HAN(in_dim=embeddings.shape[1],hid_dim=64,out_dim=3)\n",
        "out = han(graph_data.x,graph_data.edge_index)\n",
        "\n",
        "# Graph Visualization\n",
        "G = nx.Graph(); G.add_edges_from(edge_index.numpy().T)\n",
        "plt.figure(figsize=(6,6))\n",
        "nx.draw(G,node_color=\"lightblue\",with_labels=True,node_size=500)\n",
        "plt.title(\"Graph Visualization (Jobs-Firms-Reviews)\"); plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "_7QnzWyibxf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ========================================\n",
        "# Confidence prediction\n",
        "# ========================================\n",
        "softmax = nn.Softmax(dim=1)\n",
        "conf = softmax(outputs)\n",
        "print(\"Confidence Scores (sample):\",conf[:5])"
      ],
      "metadata": {
        "id": "vU8XaT6Zbu8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# 1. Load Libraries\n",
        "# ========================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re, string\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "import umap.umap_ as umap\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, RobertaTokenizer, RobertaModel, DistilBertTokenizer, DistilBertModel\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric.nn import GCNConv, HANConv, SAGEConv\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# ========================================\n",
        "# 2. Load Dataset\n",
        "# ========================================\n",
        "df = pd.read_csv(\"glassdoor_reviews.csv\")   # <-- place dataset here\n",
        "df = df[['Headline','Pros','Cons','Rating']].dropna()\n",
        "\n",
        "# Map labels: 1-2=Negative, 3=No Opinion, 4-5=Positive\n",
        "def map_sentiment(r):\n",
        "    if r <= 2: return \"Negative\"\n",
        "    elif r == 3: return \"No Opinion\"\n",
        "    else: return \"Positive\"\n",
        "\n",
        "df['Label'] = df['Rating'].apply(map_sentiment)\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "# ========================================\n",
        "# 3. Text Preprocessing\n",
        "# ========================================\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = \" \".join([w for w in text.split() if w not in stop_words])\n",
        "    return text\n",
        "\n",
        "df['text'] = df['Headline'] + \" \" + df['Pros'] + \" \" + df['Cons']\n",
        "df['text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# Train-Test Split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df['Label_enc'] = le.fit_transform(df['Label'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['Label_enc'], test_size=0.2, random_state=42)\n",
        "\n",
        "# ========================================\n",
        "# 4. Baseline Models: LSTM / BiLSTM\n",
        "# ========================================\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense\n",
        "\n",
        "max_words = 10000\n",
        "max_len = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_len)\n",
        "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_len)\n",
        "\n",
        "# ----- LSTM -----\n",
        "model_lstm = Sequential([\n",
        "    Embedding(max_words, 128, input_length=max_len),\n",
        "    LSTM(128),\n",
        "    Dense(3, activation=\"softmax\")\n",
        "])\n",
        "model_lstm.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "history_lstm = model_lstm.fit(X_train_seq, y_train, epochs=3, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# ----- BiLSTM -----\n",
        "model_bilstm = Sequential([\n",
        "    Embedding(max_words, 128, input_length=max_len),\n",
        "    Bidirectional(LSTM(128)),\n",
        "    Dense(3, activation=\"softmax\")\n",
        "])\n",
        "model_bilstm.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "history_bilstm = model_bilstm.fit(X_train_seq, y_train, epochs=3, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# ========================================\n",
        "# 5. Transformer Models: BERT, RoBERTa, DistilBERT\n",
        "# ========================================\n",
        "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "def train_transformer(model_name, train_texts, train_labels, test_texts, test_labels):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=128)\n",
        "    test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "    class TextDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, encodings, labels):\n",
        "            self.encodings = encodings\n",
        "            self.labels = labels\n",
        "        def __len__(self): return len(self.labels)\n",
        "        def __getitem__(self, idx):\n",
        "            item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "            item['labels'] = torch.tensor(self.labels[idx])\n",
        "            return item\n",
        "\n",
        "    train_dataset = TextDataset(train_encodings, list(train_labels))\n",
        "    test_dataset = TextDataset(test_encodings, list(test_labels))\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=2,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=\"./logs\",\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    preds = trainer.predict(test_dataset)\n",
        "    y_pred = np.argmax(preds.predictions, axis=1)\n",
        "    print(classification_report(test_labels, y_pred, target_names=le.classes_))\n",
        "\n",
        "# Run BERT, RoBERTa, DistilBERT\n",
        "train_transformer(\"bert-base-uncased\", X_train, y_train, X_test, y_test)\n",
        "train_transformer(\"roberta-base\", X_train, y_train, X_test, y_test)\n",
        "train_transformer(\"distilbert-base-uncased\", X_train, y_train, X_test, y_test)\n",
        "\n",
        "# ========================================\n",
        "# 6. HAN, GraphSAGE, Proposed GNN\n",
        "# ========================================\n",
        "\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class SimpleGNN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(SimpleGNN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# ========================================\n",
        "# 7. Evaluation: Confusion Matrix + Reports\n",
        "# ========================================\n",
        "def plot_confusion(y_true, y_pred, title):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.show()\n",
        "\n",
        "# ========================================\n",
        "# 8. Visualizations: t-SNE, UMAP\n",
        "# ========================================\n",
        "def plot_embeddings(X, y, method=\"tsne\"):\n",
        "    if method==\"tsne\":\n",
        "        emb = TSNE(n_components=2).fit_transform(X)\n",
        "    else:\n",
        "        emb = umap.UMAP(n_components=2).fit_transform(X)\n",
        "    plt.scatter(emb[:,0], emb[:,1], c=y, cmap=\"coolwarm\", alpha=0.7)\n",
        "    plt.title(f\"{method.upper()} Visualization\")\n",
        "    plt.show()\n",
        "\n",
        "# ========================================\n",
        "# 9. Confidence Predictions + Sample Job Recommendations\n",
        "# ========================================\n",
        "sample_texts = [\"Great culture and work life balance\",\n",
        "                \"Toxic management and very stressful\",\n",
        "                \"Decent company but long hours\"]\n",
        "\n",
        "# Use fine-tuned BERT for demonstration\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "encodings = tokenizer(sample_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "outputs = model(**encodings)\n",
        "probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "preds = torch.argmax(probs, axis=1)\n",
        "\n",
        "for txt, p, pr in zip(sample_texts, preds, probs):\n",
        "    print(f\"Text: {txt}\")\n",
        "    print(f\"Prediction: {le.classes_[p]}\")\n",
        "    print(f\"Confidence: {pr[p].item():.2f}\")\n",
        "    print(\"-\"*40)\n"
      ],
      "metadata": {
        "id": "11HNiNb9eW_y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}